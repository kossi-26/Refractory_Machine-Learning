{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72kT2NZ2fRrf"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing - Rwotolara Innocent\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "print('Shape after removing duplicates:', df.shape)\n",
        "\n",
        "# Handle outliers in key numerical features\n",
        "for col in ['Age at enrollment', 'Admission grade', 'School_Distance_km']:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "    print(f'Shape after removing outliers in {col}: {df.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing - Rwotolara Innocent\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "print('Shape after removing duplicates:', df.shape)\n",
        "\n",
        "# Handle outliers in key numerical features\n",
        "for col in ['Age at enrollment', 'Admission grade', 'School_Distance_km']:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "    print(f'Shape after removing outliers in {col}: {df.shape}')"
      ],
      "metadata": {
        "id": "tGHBsijpf8p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA Implementation - Rwotolara Innocent\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print('Shape after PCA:', X_pca.shape)\n",
        "print('Variance retained:', pca.explained_variance_ratio_.sum()).round(3)"
      ],
      "metadata": {
        "id": "5cRquoIIgAub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot PCA elbow curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Elbow Curve')\n",
        "plt.grid(True)\n",
        "plt.savefig('pca_elbow_curve.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fCe2-TlEgBeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation (PCA)** - The first few components capture most of the important structure in the data, with diminishing returns as more components are added. The “elbow” around 10-12 components suggests an optimal balance, providing most of the information without unnecessary dimensions. By 15-20 components, over 80% of the variance is already explained, so including more offers little additional benefit."
      ],
      "metadata": {
        "id": "SLU_GCl6gKGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA Implementation - Rwotolara Innocent\n",
        "# Feature Importance Ranking After PCA\n",
        "feature_names = X_encoded.columns.tolist()\n",
        "\n",
        "feature_importance = np.sum(np.abs(pca.components_), axis=0)\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": feature_importance\n",
        "}).sort_values(\"Importance\", ascending=True)\n",
        "\n",
        "# Plot features ranked by importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color=\"skyblue\")\n",
        "plt.xlabel(\"Feature Importance\", fontsize=12)\n",
        "plt.title(\"Original Features Ranked by Importance After PCA\", fontsize=14, fontweight=\"bold\")\n",
        "plt.grid(axis=\"x\", alpha=0.3, linestyle=\"--\")\n",
        "plt.savefig(\"pca_feature_importance.png\", bbox_inches=\"tight\", dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n7Lqh1zXgMnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation (PCA Feature Importance)** - This plot shows the original features ranked by their contribution to the principal components retained in PCA. Features with higher importance scores have a stronger influence on the transformed PCA space used for modeling. Both academic and non-academic factors contribute, highlighting which variables drive the variance in the dataset and are most influential in predicting student outcomes."
      ],
      "metadata": {
        "id": "yBelPXIXgRVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing - Rwotolara Innocent\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "N4gbaQh7gSK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}