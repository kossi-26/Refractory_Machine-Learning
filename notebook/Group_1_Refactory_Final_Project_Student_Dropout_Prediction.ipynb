{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Group 1 DS & ML Final Project - Student Dropout Prediction**\n",
        "\n",
        "**Project Overview**\n",
        "\n",
        "- Build a predictive model to identify students at risk of dropping out (binary classification - Dropout vs. Graduate) using academic, demographic, and socioeconomic data.\n",
        "\n",
        "**Team Members & Contributions**\n",
        "\n",
        "- Member 1 (Imienu Charity): Project setup, data loading, data describtion, and GitHub repository management\n",
        "\n",
        "- Member 2 (Abigaba Prosper): Data exploration, visualization, and statistical analysis\n",
        "\n",
        "- Member 3 (Rwotolara Innocent): Data preprocessing, feature engineering, and PCA implementation\n",
        "\n",
        "- Member 4 (Kiwanuka Kenneth): Classical ML models, hyperparameter tuning, and ensemble methods\n",
        "\n",
        "- Member 5 (Ainembabazi Allan): Neural network development, training, and deep learning optimization\n",
        "\n",
        "- Member 6 (Ekou David): Model evaluation, interpretation, business insights, and presentation\n",
        "\n",
        "GitHub Repository: [Refractory_Machine-Learning](https://github.com/kossi-26/Refractory_Machine-Learning/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project Setup - Imienu Charity\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "import joblib\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading - Imienu Charity\n",
        "# Load the primary UCI dataset\n",
        "df = pd.read_csv('data.csv', delimiter=';')\n",
        "print(f'Original UCI data shape: {df.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering - Rwotolara Innocent\n",
        "# Add simulated Tanzania-specific features\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Rural/Urban divide\n",
        "df['Location_Type'] = np.random.choice(['Urban', 'Rural'], size=len(df), p=[0.7, 0.3])\n",
        "\n",
        "# Internet access (lower in rural areas)\n",
        "df['Internet_Access'] = np.where(\n",
        "    df['Location_Type'] == 'Urban',\n",
        "    np.clip(np.random.normal(0.6, 0.2, len(df)), 0.1, 1.0),\n",
        "    np.clip(np.random.normal(0.3, 0.15, len(df)), 0.05, 0.8)\n",
        ").round(3)\n",
        "\n",
        "# Teacher qualifications (lower in rural areas)\n",
        "df['Teacher_Quality'] = np.where(\n",
        "    df['Location_Type'] == 'Urban',\n",
        "    np.clip(np.random.normal(0.75, 0.15, len(df)), 0.4, 1.0),\n",
        "    np.clip(np.random.normal(0.5, 0.2, len(df)), 0.2, 0.9)\n",
        ").round(3)\n",
        "\n",
        "# Distance to school (greater in rural areas)\n",
        "df['School_Distance_km'] = np.where(\n",
        "    df['Location_Type'] == 'Urban',\n",
        "    np.clip(np.random.gamma(2, 0.5, len(df)), 0.5, 5),\n",
        "    np.clip(np.random.gamma(3, 2, len(df)), 1, 15)\n",
        ").round(3)\n",
        "\n",
        "print(f'Enhanced dataset shape: {df.shape}') # New features added - Location_Type, Internet_Access, Teacher_Quality, School_Distance_km\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Description - Imienu Charity\n",
        "# Show basic stats of the enhanced dataset\n",
        "df.describe().round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Dictionary: Enhanced Student Dropout Prediction Dataset\n",
        "**Source:** UCI ML Repository + Simulated Specific Features\n",
        "\n",
        "**Citation:** Realinho, V., Vieira Martins, M., Machado, J., & Baptista, L. (2021)  \n",
        "\n",
        "**DOI:** https://doi.org/10.24432/C5MC89  \n",
        "\n",
        "**Students:** 4,424 | **Features:** 36 original + 4 simulated + 1 Target\n",
        "\n",
        "## Target Variable\n",
        "| Variable | Values | Description |\n",
        "|----------|---------|-------------|\n",
        "| **Target** | 0, 1 | 0=Dropout, 1=Graduate (filtered from original 3-class target) |\n",
        "\n",
        "## New Simulated Features (Tanzania Context)\n",
        "| Variable | Type | Values/Range | Description |\n",
        "|----------|------|--------------|-------------|\n",
        "| **Location_Type** | Categorical | Urban, Rural | School location type (70% Urban, 30% Rural) |\n",
        "| **Internet_Access** | Numerical | 0.0-1.0 | Internet access quality score (0.0=None, 1.0=Perfect) |\n",
        "| **Teacher_Quality** | Numerical | 0.0-1.0 | Teacher qualification score (0.0=Poor, 1.0=Excellent) |\n",
        "| **School_Distance_km** | Numerical | 0.5-15 km | Distance from home to school in kilometers |\n",
        "\n",
        "## Original Key Variables\n",
        "- **Demographics:** Age at enrollment, Gender, Marital status, Nationality\n",
        "\n",
        "- **Academic Background:** Previous qualification, Admission grade, Course\n",
        "\n",
        "- **Academic Performance:** Curricular units approved/not approved, Grades\n",
        "\n",
        "- **Financial:** Tuition fees up to date, Scholarship holder\n",
        "\n",
        "- **Economic Context:** GDP, Unemployment rate\n",
        "\n",
        "- **Family Background:** Mother's/Father's qualification and occupation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Preprocessing - Rwotolara Innocent\n",
        "# Filter to binary targets for modeling\n",
        "df = df[df['Target'].isin(['Graduate', 'Dropout'])].copy()\n",
        "df['y'] = df['Target'].map({'Dropout': 0, 'Graduate': 1})\n",
        "X = df.drop(columns=['Target', 'y'])\n",
        "y = df['y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Exploration and Visualization - Abigaba Prosper\n",
        "# Pie chart for Target distribution (Overview of outcomes)\n",
        "plt.figure(figsize=(6, 6))\n",
        "df['Target'].value_counts().plot.pie(startangle=90,  autopct='%1.1f%%', colors=['#1f77b4', '#ff7f0e'], textprops={'fontsize': 14})\n",
        "plt.title('Student Outcomes at a Glance', fontsize=18, fontweight='bold')\n",
        "plt.ylabel('')\n",
        "plt.savefig('target_pie.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation (Univariate) - This pie chart shows the distribution of student outcomes, with the majority being graduates, indicating a class imbalance where dropouts are less frequent but critical to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Age bar plot - Abigaba Prosper\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df['Age at enrollment'], bins=6, color='skyblue', kde=False, edgecolor='black')\n",
        "plt.title('Age Distribution', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Age', fontsize=14)\n",
        "plt.ylabel('Number of Students', fontsize=14)\n",
        "plt.savefig('age_bar.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation (Univariate) - The age distribution is right-skewed, with most students in their 20s, suggesting that many are recent high school graduates. Older students are less common, which may reflect different enrollment or dropout patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Admission grade bar plot - Abigaba Prosper\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df['Admission grade'], bins=15, kde=False, color='lightgreen', edgecolor='black')\n",
        "plt.title('Admission Grades', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Grade', fontsize=14)\n",
        "plt.ylabel('Number of Students', fontsize=14)\n",
        "plt.savefig('admission_bar.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation (Univariate) - Admission grades follow a normal distribution centered around 120-140, with fewer low scores, implying the dataset captures relatively high-performing students overall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar plot for mean Admission grade by Target - Abigaba Prosper\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Target', y='Admission grade', data=df, palette='Set2', errorbar=None)  # keep default CI or set errorbar='sd' for std\n",
        "plt.title('Admission Grades vs. Graduation', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Graduation Outcome', fontsize=14)\n",
        "plt.ylabel('Average Admission Grade', fontsize=14)\n",
        "plt.savefig('admission_by_target_bar.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation (Bivariate) - Both dropouts and graduates have relatively high admission grades, with only a small difference, suggesting that while academic performance matters, other non-academic factors also play a significant role in determining graduation outcomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stacked bar for School Distance by Location Type - Abigaba Prosper\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(data=df, x='School_Distance_km', hue='Location_Type', multiple='stack', palette=['#1f77b4', '#ff7f0e'], bins=10, edgecolor='black', alpha=0.8  )\n",
        "plt.title('School Travel Distance by Location', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Distance to School (km)', fontsize=14)\n",
        "plt.ylabel('Number of Students', fontsize=14)\n",
        "plt.savefig('school_distance_stacked.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation (Multivariate) - Rural students generally travel farther to school compared to urban students, which may increase the risk of dropouts due to accessibility challenges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar plot for mean Internet Access by Target - Abigaba Prosper\n",
        "df_filtered = df[df['Target'].isin(['Dropout', 'Graduate'])].copy()\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Target', y='Internet_Access', data=df_filtered, palette='Set1', errorbar=None)\n",
        "plt.title('Internet Access by Graduation Outcome', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Graduation Outcome', fontsize=14)\n",
        "plt.ylabel('Average Internet Access Score', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('internet_by_target_bar.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation (Bivariate) - Graduation outcomes show little variation in internet access scores, indicating that factors beyond connectivity are likely more influential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Horizontal bar plot of top correlations with Target - Abigaba Prosper\n",
        "num_df = df.select_dtypes(include=['float64', 'int64'])\n",
        "corr_with_target = num_df.corr()['y'].abs().drop('y').sort_values(ascending=False)\n",
        "top_corr = corr_with_target.head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_corr.sort_values().plot(kind='barh', color=sns.color_palette('bright', len(top_corr)))\n",
        "plt.title('Top 10 Factors Influencing Graduation', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Correlation Strength', fontsize=14)\n",
        "plt.ylabel('Features', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_correlations_barh.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation (Bivariate) - These are the top 10 features most strongly associated with graduation, showing that academic factors dominate, while non-academic influences also contribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72kT2NZ2fRrf"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing - Rwotolara Innocent\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "print('Shape after removing duplicates:', df.shape)\n",
        "\n",
        "# Handle outliers in key numerical features\n",
        "for col in ['Age at enrollment', 'Admission grade', 'School_Distance_km']:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "    print(f'Shape after removing outliers in {col}: {df.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGHBsijpf8p8"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing - Rwotolara Innocent\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "print('Shape after removing duplicates:', df.shape)\n",
        "\n",
        "# Handle outliers in key numerical features\n",
        "for col in ['Age at enrollment', 'Admission grade', 'School_Distance_km']:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "    print(f'Shape after removing outliers in {col}: {df.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cRquoIIgAub"
      },
      "outputs": [],
      "source": [
        "# PCA Implementation - Rwotolara Innocent\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print('Shape after PCA:', X_pca.shape)\n",
        "print('Variance retained:', pca.explained_variance_ratio_.sum()).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCe2-TlEgBeD"
      },
      "outputs": [],
      "source": [
        "# Plot PCA elbow curve - Rwotolara Innocent\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Elbow Curve')\n",
        "plt.grid(True)\n",
        "plt.savefig('pca_elbow_curve.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLU_GCl6gKGE"
      },
      "source": [
        "**Interpretation (PCA)** - The first few components capture most of the important structure in the data, with diminishing returns as more components are added. The “elbow” around 10-12 components suggests an optimal balance, providing most of the information without unnecessary dimensions. By 15-20 components, over 80% of the variance is already explained, so including more offers little additional benefit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7Lqh1zXgMnn"
      },
      "outputs": [],
      "source": [
        "# PCA Implementation - Rwotolara Innocent\n",
        "# Feature Importance Ranking After PCA\n",
        "feature_names = X_encoded.columns.tolist()\n",
        "\n",
        "feature_importance = np.sum(np.abs(pca.components_), axis=0)\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": feature_importance\n",
        "}).sort_values(\"Importance\", ascending=True)\n",
        "\n",
        "# Plot features ranked by importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color=\"skyblue\")\n",
        "plt.xlabel(\"Feature Importance\", fontsize=12)\n",
        "plt.title(\"Original Features Ranked by Importance After PCA\", fontsize=14, fontweight=\"bold\")\n",
        "plt.grid(axis=\"x\", alpha=0.3, linestyle=\"--\")\n",
        "plt.savefig(\"pca_feature_importance.png\", bbox_inches=\"tight\", dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBelPXIXgRVS"
      },
      "source": [
        "**Interpretation (PCA Feature Importance)** - This plot shows the original features ranked by their contribution to the principal components retained in PCA. Features with higher importance scores have a stronger influence on the transformed PCA space used for modeling. Both academic and non-academic factors contribute, highlighting which variables drive the variance in the dataset and are most influential in predicting student outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4gbaQh7gSK7"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing - Rwotolara Innocent\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classical ML Models, Hyperparameter Tuning, and Ensemble Methods - Kiwanuka Kenneth\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Top-performing models for tuning\n",
        "models = {\n",
        "    'SVM': {'model': SVC(probability=True, random_state=42), 'params': {'C': [0.1, 1, 10], 'kernel': ['rbf', 'poly']}},\n",
        "    'GradientBoosting': {'model': GradientBoostingClassifier(random_state=42), 'params': {'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.05, 0.1]}},\n",
        "    'Logistic': {'model': LogisticRegression(random_state=42, max_iter=1000), 'params': {'C': [0.1, 1, 10]}},\n",
        "    'KNN': {'model': KNeighborsClassifier(), 'params': {'n_neighbors': [3, 5], 'weights': ['uniform']}}\n",
        "}\n",
        "\n",
        "results = []\n",
        "tuned_models = {}\n",
        "best_model = None\n",
        "best_f1 = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tune and train models - Kiwanuka Kenneth\n",
        "for name, info in models.items():\n",
        "    print(f'Training and tuning {name}...')\n",
        "    grid = GridSearchCV(info['model'], info['params'], cv=5, scoring='f1', n_jobs=-1)\n",
        "    grid.fit(X_train_res, y_train_res)\n",
        "\n",
        "    tuned_models[name] = grid.best_estimator_\n",
        "    preds = grid.predict(X_test_scaled)\n",
        "    probs = grid.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    prec = precision_score(y_test, preds)\n",
        "    rec = recall_score(y_test, preds)\n",
        "    f1 = f1_score(y_test, preds)\n",
        "    roc = roc_auc_score(y_test, probs)\n",
        "\n",
        "    results.append({'Model': name, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1, 'ROC_AUC': roc})\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_model = grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train stacking model - Kiwanuka Kenneth\n",
        "estimators = [(name, model) for name, model in tuned_models.items()]\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5)\n",
        "stack.fit(X_train_res, y_train_res)\n",
        "\n",
        "stack_preds = stack.predict(X_test_scaled)\n",
        "stack_probs = stack.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "stack_acc = accuracy_score(y_test, stack_preds)\n",
        "stack_prec = precision_score(y_test, stack_preds)\n",
        "stack_rec = recall_score(y_test, stack_preds)\n",
        "stack_f1 = f1_score(y_test, stack_preds)\n",
        "stack_roc = roc_auc_score(y_test, stack_probs)\n",
        "\n",
        "results.append({\n",
        "    'Model': 'Stacking',\n",
        "    'Accuracy': stack_acc,\n",
        "    'Precision': stack_prec,\n",
        "    'Recall': stack_rec,\n",
        "    'F1': stack_f1,\n",
        "    'ROC_AUC': stack_roc\n",
        "})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
